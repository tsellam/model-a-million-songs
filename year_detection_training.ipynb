{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('clean_data', 'rb') as f:\n",
    "    raw_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AR7G5I41187FB4CE6C', 'AR7G5I41187FB4CE6C', 'ARXR32B1187FB57099', 'ARXR32B1187FB57099', 'AR8ZCNI1187B9A069B', 'ARIK43K1187B9AE54C', 'ARIK43K1187B9AE54C', 'ARIK43K1187B9AE54C', 'ARD842G1187B997376']\n",
      "[1982, 1982, 2007, 2007, 1984, 1986, 1986, 1986, 1987]\n"
     ]
    }
   ],
   "source": [
    "years = raw_data['year']\n",
    "artists = raw_data['artist']\n",
    "features = raw_data['features']\n",
    "print artists[1:10]\n",
    "print years[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9096, 200, 12)\n",
      "(9096,)\n",
      "['ARMJAGH1187FB546F3' 'AR7G5I41187FB4CE6C' 'AR7G5I41187FB4CE6C'\n",
      " 'ARXR32B1187FB57099' 'ARXR32B1187FB57099']\n",
      "(9096,)\n",
      "[1969 1982 1982 2007 2007]\n",
      "['AR00MBZ1187B9B5DB1' 'AR00MBZ1187B9B5DB1' 'AR00MBZ1187B9B5DB1'\n",
      " 'AR01IP11187B9AF5D2' 'AR01IP11187B9AF5D2' 'AR01IP11187B9AF5D2'\n",
      " 'AR01IP11187B9AF5D2' 'AR01IP11187B9AF5D2' 'AR01W2D1187FB5912F'\n",
      " 'AR01W2D1187FB5912F' 'AR02IU11187FB513F2' 'AR02IU11187FB513F2'\n",
      " 'AR02IU11187FB513F2' 'AR02IU11187FB513F2' 'AR02IU11187FB513F2'\n",
      " 'AR02IU11187FB513F2' 'AR02IU11187FB513F2' 'AR02IU11187FB513F2'\n",
      " 'AR02IU11187FB513F2' 'AR02IU11187FB513F2' 'AR02IU11187FB513F2'\n",
      " 'AR02IU11187FB513F2' 'AR02IU11187FB513F2' 'AR02KZG1187FB3B3B4'\n",
      " 'AR02KZG1187FB3B3B4' 'AR02KZG1187FB3B3B4' 'AR02KZG1187FB3B3B4'\n",
      " 'AR02KZG1187FB3B3B4' 'AR02YGA1187B9B8AC4' 'AR02YGA1187B9B8AC4'\n",
      " 'AR02YGA1187B9B8AC4' 'AR02YGA1187B9B8AC4' 'AR02YGA1187B9B8AC4'\n",
      " 'AR02YGA1187B9B8AC4' 'AR02YGA1187B9B8AC4' 'AR02YGA1187B9B8AC4'\n",
      " 'AR02YGA1187B9B8AC4' 'AR035N21187FB3938E' 'AR035N21187FB3938E'\n",
      " 'AR035N21187FB3938E' 'AR035N21187FB3938E' 'AR035N21187FB3938E'\n",
      " 'AR035N21187FB3938E' 'AR035N21187FB3938E' 'AR035N21187FB3938E'\n",
      " 'AR035N21187FB3938E' 'AR035N21187FB3938E' 'AR035N21187FB3938E'\n",
      " 'AR035N21187FB3938E' 'AR035N21187FB3938E']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creates the arrays\n",
    "X = np.stack(features)\n",
    "X = np.transpose(X, (0,2,1))\n",
    "print(X.shape)\n",
    "\n",
    "y_artists = np.array(artists)\n",
    "print y_artists.shape\n",
    "print y_artists[0:5]\n",
    "\n",
    "y_years = np.array(years)\n",
    "print y_years.shape\n",
    "print y_years[0:5]\n",
    "\n",
    "order = np.argsort(y_artists)\n",
    "X = X[order,...]\n",
    "y_artists = y_artists[order,...]\n",
    "y_years = y_years[order,...]\n",
    "\n",
    "print y_artists[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8186, 200, 12) (8186,)\n",
      "(910, 200, 12) (910,)\n"
     ]
    }
   ],
   "source": [
    "TEST_RATIO = .1\n",
    "\n",
    "n_train = X.shape[0] * (1-TEST_RATIO)\n",
    "n_train = int(n_train)\n",
    "\n",
    "X_train = X[:n_train]\n",
    "X_test =  X[n_train:]\n",
    "\n",
    "y_train = y_years[:n_train]\n",
    "y_test =  y_years[n_train:]\n",
    "\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAHgCAYAAAB91L6VAAAEDWlDQ1BJQ0MgUHJvZmlsZQAAOI2N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\nAQQQQMB1AgRg1+0SKoQAAgggkAoC/w9uhh/tQXxkrgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i y_train\n",
    "hist(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAHgCAYAAAB91L6VAAAEDWlDQ1BJQ0MgUHJvZmlsZQAAOI2N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\nRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i y_test\n",
    "hist(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 1: plain stupid regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8186, 12) (910, 12)\n"
     ]
    }
   ],
   "source": [
    "flat_X_train = np.mean(X_train, axis=1)\n",
    "flat_X_test  = np.mean(X_test,  axis=1)\n",
    "\n",
    "print flat_X_train.shape, flat_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, gcv_mode=None,\n",
       "    normalize=False, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "reg.fit(flat_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.199780325452\n",
      "Squared loss 100505.833278\n",
      "RMSE 317.0265498\n",
      "Average error 7.93224941011\n",
      "*****\n",
      "*****\n",
      "2000.06932379 1995\n",
      "1999.29038116 1997\n",
      "1999.80902808 1991\n",
      "1995.33908219 2008\n",
      "1999.8392717 2008\n",
      "1997.78226852 2008\n",
      "1999.82310699 2006\n",
      "1999.06954812 2006\n",
      "1999.14111729 2008\n",
      "1999.8880894 2008\n",
      "1997.54349254 1972\n",
      "1996.87818873 1972\n",
      "1999.45155362 1993\n",
      "2001.21728469 1998\n",
      "1997.44622364 1993\n",
      "1999.67099285 1993\n",
      "1999.46813548 1998\n",
      "1990.11201451 1998\n",
      "2004.50243501 1987\n",
      "1996.60862501 1987\n",
      "2004.86286121 1987\n",
      "2003.02925529 1998\n",
      "1994.17319452 1993\n",
      "1994.27891521 2003\n",
      "1993.84333564 2003\n"
     ]
    }
   ],
   "source": [
    "print 'R^2:', reg.score(flat_X_test, y_test)\n",
    "\n",
    "t = reg.predict(flat_X_test)\n",
    "sl = np.sum((t - y_test)**2)\n",
    "print 'Squared loss',sl\n",
    "print 'RMSE', np.sqrt(sl)\n",
    "print 'Average error', np.mean(np.abs(t - y_test))\n",
    "print('*****')\n",
    "print('*****')\n",
    "for i in range(25):\n",
    "    print t[i], y_train[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fancier regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8186, 2400) (910, 2400)\n",
      "(8186,) (910,)\n"
     ]
    }
   ],
   "source": [
    "flat_X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "flat_X_test  = np.reshape(X_test, (X_test.shape[0], X_test.shape[1] * X_test.shape[2]))\n",
    "print flat_X_train.shape, flat_X_test.shape\n",
    "print y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, gcv_mode=None,\n",
       "    normalize=False, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg2 = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "reg2.fit(flat_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: -0.156739065808\n",
      "Squared loss 145283.885653\n",
      "RMSE 381.161233145\n",
      "Average error 9.62914901213\n",
      "*****\n",
      "*****\n",
      "1994.26893543 1995\n",
      "2004.32458996 1997\n",
      "1997.72187624 1991\n",
      "1988.35271891 2008\n",
      "1983.5189163 2008\n",
      "1999.19569924 2008\n",
      "2010.48593728 2006\n",
      "2001.18502431 2006\n",
      "1999.98923624 2008\n",
      "1992.29957257 2008\n",
      "1993.71898147 1972\n",
      "2001.78826938 1972\n",
      "1996.7891817 1993\n",
      "1997.02093411 1998\n",
      "1999.20505878 1993\n",
      "1998.89708353 1993\n",
      "1985.2176006 1998\n",
      "1983.43620084 1998\n",
      "2002.14536179 1987\n",
      "1991.891566 1987\n",
      "2003.64626643 1987\n",
      "1996.02120472 1998\n",
      "1998.12168245 1993\n",
      "1994.60284193 2003\n",
      "1992.71130919 2003\n"
     ]
    }
   ],
   "source": [
    "print 'R^2:', reg2.score(flat_X_test, y_test)\n",
    "\n",
    "t = reg2.predict(flat_X_test)\n",
    "sl = np.sum((t - y_test)**2)\n",
    "print 'Squared loss',sl\n",
    "print 'RMSE', np.sqrt(sl)\n",
    "print 'Average error', np.mean(np.abs(t - y_test))\n",
    "print('*****')\n",
    "print('*****')\n",
    "for i in range(25):\n",
    "    print t[i], y_train[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 2: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8186, 2400) (910, 2400)\n",
      "(8186,) (910,)\n"
     ]
    }
   ],
   "source": [
    "flat_X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1] * X_train.shape[2]))\n",
    "flat_X_test  = np.reshape(X_test, (X_test.shape[0], X_test.shape[1] * X_test.shape[2]))\n",
    "print flat_X_train.shape, flat_X_test.shape\n",
    "print y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "************* 8 *************\n",
      "[<keras.layers.core.Dense object at 0x129ec05d0>, <keras.layers.core.Activation object at 0x129ed3990>, <keras.layers.core.Dense object at 0x129ed3850>]\n",
      "Epoch 1/160\n",
      "8186/8186 [==============================] - 2s - loss: 24.7944     \n",
      "Epoch 2/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.2757     \n",
      "Epoch 3/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0256     \n",
      "Epoch 4/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 5/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 6/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 7/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0000     \n",
      "Epoch 8/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 9/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 10/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 11/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 12/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 13/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 14/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 15/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 16/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 17/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 18/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 19/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 20/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 21/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 22/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 23/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 24/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 25/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 26/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 27/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 28/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 29/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 30/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 31/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 32/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 33/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 34/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 35/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 36/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 37/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 38/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 39/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 40/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 41/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 42/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 43/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 44/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 45/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 46/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 47/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 48/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 49/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 50/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 51/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 52/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 53/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 54/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 55/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 56/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 57/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 58/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 59/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 60/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 61/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 62/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 63/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 64/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 65/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 66/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 67/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 68/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 69/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 70/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 71/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 72/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 73/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 74/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 75/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 76/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 77/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 78/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 79/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 80/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 81/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 82/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 83/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 84/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 85/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 86/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 87/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 88/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 89/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 90/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0000     \n",
      "Epoch 91/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 92/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 93/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 94/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 95/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 96/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 97/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 98/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 99/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 100/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 101/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 102/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 103/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 104/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 105/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 106/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 107/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 108/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 109/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 110/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0000     \n",
      "Epoch 111/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 112/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 113/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 114/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 115/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 116/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 117/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 118/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 119/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 120/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 121/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 122/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 123/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 124/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 125/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 126/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 127/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 128/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 129/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 130/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 131/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 132/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 133/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0000     \n",
      "Epoch 134/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 135/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 136/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 137/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 138/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 139/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 140/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 141/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 142/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 143/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 144/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 145/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 146/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0000     \n",
      "Epoch 147/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 148/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 149/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 150/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 151/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 152/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 153/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 154/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 155/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 156/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0001     \n",
      "Epoch 157/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 158/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 159/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "Epoch 160/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0002     \n",
      "128/910 [===>..........................] - ETA: 2s1.26930656145\n",
      "Squared loss 125277675.19\n",
      "RMSE 11192.7510108\n",
      "Average error 9.26665584185\n",
      "*****\n",
      "[ 1997.64550781] 1995\n",
      "[ 1997.64550781] 1997\n",
      "[ 1997.64550781] 1991\n",
      "[ 1997.64550781] 2008\n",
      "[ 1997.64550781] 2008\n",
      "[ 1997.64550781] 2008\n",
      "[ 1997.64550781] 2006\n",
      "[ 1997.64550781] 2006\n",
      "[ 1997.64550781] 2008\n",
      "[ 1997.64550781] 2008\n",
      "[ 1997.64550781] 1972\n",
      "[ 1997.64550781] 1972\n",
      "[ 1997.64550781] 1993\n",
      "[ 1997.64550781] 1998\n",
      "[ 1997.64550781] 1993\n",
      "************* 16 *************\n",
      "[<keras.layers.core.Dense object at 0x12b4aed90>, <keras.layers.core.Activation object at 0x129ed3710>, <keras.layers.core.Dense object at 0x127c8ae90>]\n",
      "Epoch 1/160\n",
      "8186/8186 [==============================] - 1s - loss: 72.5527     \n",
      "Epoch 2/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.1010     \n",
      "Epoch 3/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.9996     \n",
      "Epoch 4/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.9771     \n",
      "Epoch 5/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.9437     \n",
      "Epoch 6/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.9010     \n",
      "Epoch 7/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.8732     \n",
      "Epoch 8/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.8378     \n",
      "Epoch 9/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.8133     \n",
      "Epoch 10/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7901     \n",
      "Epoch 11/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7705     \n",
      "Epoch 12/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7519     \n",
      "Epoch 13/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7331     \n",
      "Epoch 14/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7172     \n",
      "Epoch 15/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7102     \n",
      "Epoch 16/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6966     \n",
      "Epoch 17/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6848     \n",
      "Epoch 18/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6780     \n",
      "Epoch 19/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6727     \n",
      "Epoch 20/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6560     \n",
      "Epoch 21/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6565     \n",
      "Epoch 22/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6521     \n",
      "Epoch 23/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6418     \n",
      "Epoch 24/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6357     \n",
      "Epoch 25/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6343     \n",
      "Epoch 26/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6297     \n",
      "Epoch 27/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6220     \n",
      "Epoch 28/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6106     \n",
      "Epoch 29/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6085     \n",
      "Epoch 30/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6134     \n",
      "Epoch 31/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 0s - loss: 0.6015     \n",
      "Epoch 32/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6049     \n",
      "Epoch 33/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6003     \n",
      "Epoch 34/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6001     \n",
      "Epoch 35/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5986     \n",
      "Epoch 36/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5961     \n",
      "Epoch 37/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5971     \n",
      "Epoch 38/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5930     \n",
      "Epoch 39/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5874     \n",
      "Epoch 40/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5888     \n",
      "Epoch 41/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5864     \n",
      "Epoch 42/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5853     \n",
      "Epoch 43/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5804     \n",
      "Epoch 44/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5831     \n",
      "Epoch 45/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5792     \n",
      "Epoch 46/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5790     \n",
      "Epoch 47/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5760     \n",
      "Epoch 48/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5742     \n",
      "Epoch 49/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5760     \n",
      "Epoch 50/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5750     \n",
      "Epoch 51/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5751     \n",
      "Epoch 52/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5714     \n",
      "Epoch 53/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5698     \n",
      "Epoch 54/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5706     \n",
      "Epoch 55/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5683     \n",
      "Epoch 56/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5673     \n",
      "Epoch 57/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5675     \n",
      "Epoch 58/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5660     \n",
      "Epoch 59/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5645     \n",
      "Epoch 60/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5644     \n",
      "Epoch 61/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5648     \n",
      "Epoch 62/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5627     \n",
      "Epoch 63/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5601     \n",
      "Epoch 64/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5635     \n",
      "Epoch 65/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5604     \n",
      "Epoch 66/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5593     \n",
      "Epoch 67/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5608     \n",
      "Epoch 68/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5581     \n",
      "Epoch 69/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5605     \n",
      "Epoch 70/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5594     \n",
      "Epoch 71/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5569     \n",
      "Epoch 72/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5576     \n",
      "Epoch 73/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5562     \n",
      "Epoch 74/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5562     \n",
      "Epoch 75/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5562     \n",
      "Epoch 76/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5542     \n",
      "Epoch 77/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5555     \n",
      "Epoch 78/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5532     \n",
      "Epoch 79/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5543     \n",
      "Epoch 80/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5540     \n",
      "Epoch 81/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5530     \n",
      "Epoch 82/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5556     \n",
      "Epoch 83/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5522     \n",
      "Epoch 84/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5516     \n",
      "Epoch 85/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5528     \n",
      "Epoch 86/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5505     \n",
      "Epoch 87/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5504     \n",
      "Epoch 88/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5523     \n",
      "Epoch 89/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5494     \n",
      "Epoch 90/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5522     \n",
      "Epoch 91/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5495     \n",
      "Epoch 92/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5508     \n",
      "Epoch 93/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5500     \n",
      "Epoch 94/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5525     \n",
      "Epoch 95/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5478     \n",
      "Epoch 96/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5489     \n",
      "Epoch 97/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5501     \n",
      "Epoch 98/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5499     \n",
      "Epoch 99/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5478     \n",
      "Epoch 100/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5475     \n",
      "Epoch 101/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5474     \n",
      "Epoch 102/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5460     \n",
      "Epoch 103/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5472     \n",
      "Epoch 104/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5456     \n",
      "Epoch 105/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5455     \n",
      "Epoch 106/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5465     \n",
      "Epoch 107/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5436     \n",
      "Epoch 108/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5433     \n",
      "Epoch 109/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5448     \n",
      "Epoch 110/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5454     \n",
      "Epoch 111/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5417     \n",
      "Epoch 112/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5425     \n",
      "Epoch 113/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5433     \n",
      "Epoch 114/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5421     \n",
      "Epoch 115/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5431     \n",
      "Epoch 116/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5409     \n",
      "Epoch 117/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5408     \n",
      "Epoch 118/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5414     \n",
      "Epoch 119/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5413     \n",
      "Epoch 120/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5422     \n",
      "Epoch 121/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5402     \n",
      "Epoch 122/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5404     \n",
      "Epoch 123/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5391     \n",
      "Epoch 124/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5403     \n",
      "Epoch 125/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5400     \n",
      "Epoch 126/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5390     \n",
      "Epoch 127/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5396     \n",
      "Epoch 128/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5413     \n",
      "Epoch 129/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5398     \n",
      "Epoch 130/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5391     \n",
      "Epoch 131/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 0s - loss: 0.5390     \n",
      "Epoch 132/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5393     \n",
      "Epoch 133/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5393     \n",
      "Epoch 134/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5391     \n",
      "Epoch 135/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5378     \n",
      "Epoch 136/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5393     \n",
      "Epoch 137/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5380     \n",
      "Epoch 138/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5377     \n",
      "Epoch 139/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5383     \n",
      "Epoch 140/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5367     \n",
      "Epoch 141/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5373     \n",
      "Epoch 142/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5362     \n",
      "Epoch 143/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5379     \n",
      "Epoch 144/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5362     \n",
      "Epoch 145/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5379     \n",
      "Epoch 146/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5363     \n",
      "Epoch 147/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5367     \n",
      "Epoch 148/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5364     \n",
      "Epoch 149/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5346     \n",
      "Epoch 150/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5361     \n",
      "Epoch 151/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5359     \n",
      "Epoch 152/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5363     \n",
      "Epoch 153/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5367     \n",
      "Epoch 154/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5358     \n",
      "Epoch 155/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5353     \n",
      "Epoch 156/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5357     \n",
      "Epoch 157/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5346     \n",
      "Epoch 158/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5352     \n",
      "Epoch 159/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5360     \n",
      "Epoch 160/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5356     \n",
      "128/910 [===>..........................] - ETA: 2s1.6428820893\n",
      "Squared loss 177143779.434\n",
      "RMSE 13309.5371608\n",
      "Average error 10.3842059359\n",
      "*****\n",
      "[ 2000.05432129] 1995\n",
      "[ 2000.05432129] 1997\n",
      "[ 1974.54101562] 1991\n",
      "[ 2000.05432129] 2008\n",
      "[ 2000.05432129] 2008\n",
      "[ 2000.05432129] 2008\n",
      "[ 2000.05432129] 2006\n",
      "[ 2000.05432129] 2006\n",
      "[ 1986.45458984] 2008\n",
      "[ 1998.11450195] 2008\n",
      "[ 2000.05432129] 1972\n",
      "[ 1969.86315918] 1972\n",
      "[ 2000.05432129] 1993\n",
      "[ 2000.05432129] 1998\n",
      "[ 2000.05432129] 1993\n",
      "************* 32 *************\n",
      "[<keras.layers.core.Dense object at 0x12abfeb50>, <keras.layers.core.Activation object at 0x12ac603d0>, <keras.layers.core.Dense object at 0x12ac60510>]\n",
      "Epoch 1/160\n",
      "8186/8186 [==============================] - 1s - loss: 103.0311     \n",
      "Epoch 2/160\n",
      "8186/8186 [==============================] - 0s - loss: 5.7435     \n",
      "Epoch 3/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.2959     \n",
      "Epoch 4/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.9319     \n",
      "Epoch 5/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.8794     \n",
      "Epoch 6/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.8383     \n",
      "Epoch 7/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7991     \n",
      "Epoch 8/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7717     \n",
      "Epoch 9/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7491     \n",
      "Epoch 10/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7279     \n",
      "Epoch 11/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7190     \n",
      "Epoch 12/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6950     \n",
      "Epoch 13/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6837     \n",
      "Epoch 14/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6710     \n",
      "Epoch 15/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6570     \n",
      "Epoch 16/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6529     \n",
      "Epoch 17/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6468     \n",
      "Epoch 18/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6288     \n",
      "Epoch 19/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6225     \n",
      "Epoch 20/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6120     \n",
      "Epoch 21/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6064     \n",
      "Epoch 22/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6050     \n",
      "Epoch 23/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5978     \n",
      "Epoch 24/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5964     \n",
      "Epoch 25/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5887     \n",
      "Epoch 26/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5831     \n",
      "Epoch 27/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5780     \n",
      "Epoch 28/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5796     \n",
      "Epoch 29/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5727     \n",
      "Epoch 30/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5647     \n",
      "Epoch 31/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5682     \n",
      "Epoch 32/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5623     \n",
      "Epoch 33/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5599     \n",
      "Epoch 34/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5564     \n",
      "Epoch 35/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5578     \n",
      "Epoch 36/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5462     \n",
      "Epoch 37/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5503     \n",
      "Epoch 38/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5462     \n",
      "Epoch 39/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5488     \n",
      "Epoch 40/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5405     \n",
      "Epoch 41/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5404     \n",
      "Epoch 42/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5382     \n",
      "Epoch 43/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5378     \n",
      "Epoch 44/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5365     \n",
      "Epoch 45/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5382     \n",
      "Epoch 46/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5326     \n",
      "Epoch 47/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5287     \n",
      "Epoch 48/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5289     \n",
      "Epoch 49/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5285     \n",
      "Epoch 50/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5231     \n",
      "Epoch 51/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5253     \n",
      "Epoch 52/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5236     \n",
      "Epoch 53/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5239     \n",
      "Epoch 54/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5215     \n",
      "Epoch 55/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5137     \n",
      "Epoch 56/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5145     \n",
      "Epoch 57/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5228     \n",
      "Epoch 58/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5132     \n",
      "Epoch 59/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5178     \n",
      "Epoch 60/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5126     \n",
      "Epoch 61/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5136     \n",
      "Epoch 62/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5107     \n",
      "Epoch 63/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 0s - loss: 0.5125     \n",
      "Epoch 64/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5072     \n",
      "Epoch 65/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5134     \n",
      "Epoch 66/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5100     \n",
      "Epoch 67/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5067     \n",
      "Epoch 68/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5125     \n",
      "Epoch 69/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5072     \n",
      "Epoch 70/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5064     \n",
      "Epoch 71/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5080     \n",
      "Epoch 72/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5049     \n",
      "Epoch 73/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5040     \n",
      "Epoch 74/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5061     \n",
      "Epoch 75/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5034     \n",
      "Epoch 76/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5027     \n",
      "Epoch 77/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5009     \n",
      "Epoch 78/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5012     \n",
      "Epoch 79/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5039     \n",
      "Epoch 80/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4987     \n",
      "Epoch 81/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4996     \n",
      "Epoch 82/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5003     \n",
      "Epoch 83/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5004     \n",
      "Epoch 84/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4972     \n",
      "Epoch 85/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4998     \n",
      "Epoch 86/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4983     \n",
      "Epoch 87/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5011     \n",
      "Epoch 88/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4954     \n",
      "Epoch 89/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4967     \n",
      "Epoch 90/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4980     \n",
      "Epoch 91/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4958     \n",
      "Epoch 92/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4933     \n",
      "Epoch 93/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4949     \n",
      "Epoch 94/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4930     \n",
      "Epoch 95/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4969     \n",
      "Epoch 96/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4915     \n",
      "Epoch 97/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4950     \n",
      "Epoch 98/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4920     \n",
      "Epoch 99/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4905     \n",
      "Epoch 100/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4953     \n",
      "Epoch 101/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4900     \n",
      "Epoch 102/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4941     \n",
      "Epoch 103/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4903     \n",
      "Epoch 104/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4902     \n",
      "Epoch 105/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4887     \n",
      "Epoch 106/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4923     \n",
      "Epoch 107/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4929     \n",
      "Epoch 108/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4906     \n",
      "Epoch 109/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4914     \n",
      "Epoch 110/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4885     \n",
      "Epoch 111/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4891     \n",
      "Epoch 112/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4896     \n",
      "Epoch 113/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4872     \n",
      "Epoch 114/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4870     \n",
      "Epoch 115/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4903     \n",
      "Epoch 116/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4874     \n",
      "Epoch 117/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4884     \n",
      "Epoch 118/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4874     \n",
      "Epoch 119/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4891     \n",
      "Epoch 120/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4865     \n",
      "Epoch 121/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4847     \n",
      "Epoch 122/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4879     \n",
      "Epoch 123/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4871     \n",
      "Epoch 124/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4839     \n",
      "Epoch 125/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4842     \n",
      "Epoch 126/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4851     \n",
      "Epoch 127/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4859     \n",
      "Epoch 128/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4812     \n",
      "Epoch 129/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4837     \n",
      "Epoch 130/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4832     \n",
      "Epoch 131/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4808     \n",
      "Epoch 132/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4843     \n",
      "Epoch 133/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4830     \n",
      "Epoch 134/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4822     \n",
      "Epoch 135/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4857     \n",
      "Epoch 136/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4806     \n",
      "Epoch 137/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4845     \n",
      "Epoch 138/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4803     \n",
      "Epoch 139/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4830     \n",
      "Epoch 140/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4824     \n",
      "Epoch 141/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4808     \n",
      "Epoch 142/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4824     \n",
      "Epoch 143/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4790     \n",
      "Epoch 144/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4783     \n",
      "Epoch 145/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4800     \n",
      "Epoch 146/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4797     \n",
      "Epoch 147/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4787     \n",
      "Epoch 148/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4827     \n",
      "Epoch 149/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4814     \n",
      "Epoch 150/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4781     \n",
      "Epoch 151/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4821     \n",
      "Epoch 152/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4766     \n",
      "Epoch 153/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4776     \n",
      "Epoch 154/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4774     \n",
      "Epoch 155/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4810     \n",
      "Epoch 156/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4764     \n",
      "Epoch 157/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4787     \n",
      "Epoch 158/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4807     \n",
      "Epoch 159/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4778     \n",
      "Epoch 160/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.4782     \n",
      "128/910 [===>..........................] - ETA: 3s1.8272915138\n",
      "Squared loss 205969071.688\n",
      "RMSE 14351.6226152\n",
      "Average error 10.9465214507\n",
      "*****\n",
      "[ 2000.43908691] 1995\n",
      "[ 2000.43908691] 1997\n",
      "[ 1974.56738281] 1991\n",
      "[ 2000.43908691] 2008\n",
      "[ 2000.43908691] 2008\n",
      "[ 2000.43908691] 2008\n",
      "[ 1960.54077148] 2006\n",
      "[ 1982.89880371] 2006\n",
      "[ 1970.34985352] 2008\n",
      "[ 1988.16992188] 2008\n",
      "[ 2000.43908691] 1972\n",
      "[ 1972.39746094] 1972\n",
      "[ 2000.43908691] 1993\n",
      "[ 2000.43908691] 1998\n",
      "[ 2000.43908691] 1993\n",
      "************* 64 *************\n",
      "[<keras.layers.core.Dense object at 0x12b396d50>, <keras.layers.core.Activation object at 0x12b5a6510>, <keras.layers.core.Dense object at 0x12b68f4d0>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "8186/8186 [==============================] - 1s - loss: 265.5228     \n",
      "Epoch 2/160\n",
      "8186/8186 [==============================] - 0s - loss: 4.7741     \n",
      "Epoch 3/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.7767     \n",
      "Epoch 4/160\n",
      "8186/8186 [==============================] - 0s - loss: 1.0552     \n",
      "Epoch 5/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.9537     \n",
      "Epoch 6/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.9215     \n",
      "Epoch 7/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.8911     \n",
      "Epoch 8/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.8459     \n",
      "Epoch 9/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.8251     \n",
      "Epoch 10/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.8037     \n",
      "Epoch 11/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7758     \n",
      "Epoch 12/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7586     \n",
      "Epoch 13/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7418     \n",
      "Epoch 14/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7180     \n",
      "Epoch 15/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.7080     \n",
      "Epoch 16/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6952     \n",
      "Epoch 17/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6856     \n",
      "Epoch 18/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6786     \n",
      "Epoch 19/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6707     \n",
      "Epoch 20/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6594     \n",
      "Epoch 21/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6523     \n",
      "Epoch 22/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6406     \n",
      "Epoch 23/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6377     \n",
      "Epoch 24/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6321     \n",
      "Epoch 25/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6253     \n",
      "Epoch 26/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6187     \n",
      "Epoch 27/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6122     \n",
      "Epoch 28/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6096     \n",
      "Epoch 29/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6073     \n",
      "Epoch 30/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5957     \n",
      "Epoch 31/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.6032     \n",
      "Epoch 32/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5977     \n",
      "Epoch 33/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5907     \n",
      "Epoch 34/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5884     \n",
      "Epoch 35/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5861     \n",
      "Epoch 36/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5770     \n",
      "Epoch 37/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5823     \n",
      "Epoch 38/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5769     \n",
      "Epoch 39/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5779     \n",
      "Epoch 40/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5744     \n",
      "Epoch 41/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5693     \n",
      "Epoch 42/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5672     \n",
      "Epoch 43/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5711     \n",
      "Epoch 44/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5664     \n",
      "Epoch 45/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5650     \n",
      "Epoch 46/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5641     \n",
      "Epoch 47/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5590     \n",
      "Epoch 48/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5621     \n",
      "Epoch 49/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5602     \n",
      "Epoch 50/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5630     \n",
      "Epoch 51/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5580     \n",
      "Epoch 52/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5509     \n",
      "Epoch 53/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5530     \n",
      "Epoch 54/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5547     \n",
      "Epoch 55/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5506     \n",
      "Epoch 56/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5513     \n",
      "Epoch 57/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5493     \n",
      "Epoch 58/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5461     \n",
      "Epoch 59/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5458     \n",
      "Epoch 60/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5469     \n",
      "Epoch 61/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5456     \n",
      "Epoch 62/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5422     \n",
      "Epoch 63/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5464     \n",
      "Epoch 64/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5442     \n",
      "Epoch 65/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5424     \n",
      "Epoch 66/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5463     \n",
      "Epoch 67/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5397     \n",
      "Epoch 68/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5380     \n",
      "Epoch 69/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5375     \n",
      "Epoch 70/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5395     \n",
      "Epoch 71/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5374     \n",
      "Epoch 72/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5336     \n",
      "Epoch 73/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5359     \n",
      "Epoch 74/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5356     \n",
      "Epoch 75/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5391     \n",
      "Epoch 76/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5355     \n",
      "Epoch 77/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5317     \n",
      "Epoch 78/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5306     \n",
      "Epoch 79/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5284     \n",
      "Epoch 80/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5286     \n",
      "Epoch 81/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5302     \n",
      "Epoch 82/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5281     \n",
      "Epoch 83/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5268     \n",
      "Epoch 84/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5303     \n",
      "Epoch 85/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5253     \n",
      "Epoch 86/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5255     \n",
      "Epoch 87/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5274     \n",
      "Epoch 88/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5284     \n",
      "Epoch 89/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5248     \n",
      "Epoch 90/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5224     \n",
      "Epoch 91/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5214     \n",
      "Epoch 92/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5263     \n",
      "Epoch 93/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5252     \n",
      "Epoch 94/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5198     \n",
      "Epoch 95/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5206     \n",
      "Epoch 96/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5232     \n",
      "Epoch 97/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5253     \n",
      "Epoch 98/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5199     \n",
      "Epoch 99/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5186     \n",
      "Epoch 100/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5202     \n",
      "Epoch 101/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5194     \n",
      "Epoch 102/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 0s - loss: 0.5185     \n",
      "Epoch 103/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5151     \n",
      "Epoch 104/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5219     \n",
      "Epoch 105/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5148     \n",
      "Epoch 106/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5230     \n",
      "Epoch 107/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5161     \n",
      "Epoch 108/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5162     \n",
      "Epoch 109/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5233     \n",
      "Epoch 110/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5148     \n",
      "Epoch 111/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5180     \n",
      "Epoch 112/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5152     \n",
      "Epoch 113/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5148     \n",
      "Epoch 114/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5168     \n",
      "Epoch 115/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5145     \n",
      "Epoch 116/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5123     \n",
      "Epoch 117/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5174     \n",
      "Epoch 118/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5137     \n",
      "Epoch 119/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5141     \n",
      "Epoch 120/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5148     \n",
      "Epoch 121/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5131     \n",
      "Epoch 122/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5150     \n",
      "Epoch 123/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5151     \n",
      "Epoch 124/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5114     \n",
      "Epoch 125/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5111     \n",
      "Epoch 126/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5094     \n",
      "Epoch 127/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5114     \n",
      "Epoch 128/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5118     \n",
      "Epoch 129/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5099     \n",
      "Epoch 130/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5150     \n",
      "Epoch 131/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5117     \n",
      "Epoch 132/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5089     \n",
      "Epoch 133/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5068     \n",
      "Epoch 134/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5072     \n",
      "Epoch 135/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5060     \n",
      "Epoch 136/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5065     \n",
      "Epoch 137/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5070     \n",
      "Epoch 138/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5091     \n",
      "Epoch 139/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5091     \n",
      "Epoch 140/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5078     \n",
      "Epoch 141/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5075     \n",
      "Epoch 142/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5081     \n",
      "Epoch 143/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5081     \n",
      "Epoch 144/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5046     \n",
      "Epoch 145/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5053     \n",
      "Epoch 146/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5083     \n",
      "Epoch 147/160\n",
      "8186/8186 [==============================] - ETA: 0s - loss: 0.504 - 0s - loss: 0.5061     \n",
      "Epoch 148/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5033     \n",
      "Epoch 149/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5058     \n",
      "Epoch 150/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5051     \n",
      "Epoch 151/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5058     \n",
      "Epoch 152/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5039     \n",
      "Epoch 153/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5048     \n",
      "Epoch 154/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5056     \n",
      "Epoch 155/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5047     \n",
      "Epoch 156/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5042     \n",
      "Epoch 157/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5007     \n",
      "Epoch 158/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5006     \n",
      "Epoch 159/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5035     \n",
      "Epoch 160/160\n",
      "8186/8186 [==============================] - 0s - loss: 0.5017     \n",
      "128/910 [===>..........................] - ETA: 3s1.9403637493\n",
      "Squared loss 207400457.593\n",
      "RMSE 14401.404709\n",
      "Average error 11.0593080124\n",
      "*****\n",
      "[ 2000.19140625] 1995\n",
      "[ 2000.19140625] 1997\n",
      "[ 2000.19140625] 1991\n",
      "[ 2000.19140625] 2008\n",
      "[ 2000.19140625] 2008\n",
      "[ 1995.11535645] 2008\n",
      "[ 1993.9732666] 2006\n",
      "[ 1999.81848145] 2006\n",
      "[ 1973.89709473] 2008\n",
      "[ 1992.60949707] 2008\n",
      "[ 2000.19140625] 1972\n",
      "[ 1985.57043457] 1972\n",
      "[ 2000.19140625] 1993\n",
      "[ 2000.19140625] 1998\n",
      "[ 2000.19140625] 1993\n",
      "************* 128 *************\n",
      "[<keras.layers.core.Dense object at 0x12c185910>, <keras.layers.core.Activation object at 0x12cdb1b10>, <keras.layers.core.Dense object at 0x12cdb1090>]\n",
      "Epoch 1/160\n",
      "8186/8186 [==============================] - 2s - loss: 565.9076     \n",
      "Epoch 2/160\n",
      "8186/8186 [==============================] - 1s - loss: 10.4310     \n",
      "Epoch 3/160\n",
      "8186/8186 [==============================] - 1s - loss: 3.3323     \n",
      "Epoch 4/160\n",
      "8186/8186 [==============================] - 1s - loss: 1.4448     \n",
      "Epoch 5/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.9657     \n",
      "Epoch 6/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.8918     \n",
      "Epoch 7/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.8616     \n",
      "Epoch 8/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.8664     \n",
      "Epoch 9/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.8931     \n",
      "Epoch 10/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.7948     \n",
      "Epoch 11/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.7671     \n",
      "Epoch 12/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.7346     \n",
      "Epoch 13/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.7176     \n",
      "Epoch 14/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.7073     \n",
      "Epoch 15/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.6960     \n",
      "Epoch 16/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.6827     \n",
      "Epoch 17/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.6590     \n",
      "Epoch 18/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.6445     \n",
      "Epoch 19/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.6412     \n",
      "Epoch 20/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.6369     \n",
      "Epoch 21/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.6225     \n",
      "Epoch 22/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.6120     \n",
      "Epoch 23/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.6113     \n",
      "Epoch 24/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5986     \n",
      "Epoch 25/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5958     \n",
      "Epoch 26/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5911     \n",
      "Epoch 27/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5934     \n",
      "Epoch 28/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5787     \n",
      "Epoch 29/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5809     \n",
      "Epoch 30/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5751     \n",
      "Epoch 31/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5679     \n",
      "Epoch 32/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5650     \n",
      "Epoch 33/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5598     \n",
      "Epoch 34/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 1s - loss: 0.5632     \n",
      "Epoch 35/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5623     \n",
      "Epoch 36/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5512     \n",
      "Epoch 37/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5500     \n",
      "Epoch 38/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5477     \n",
      "Epoch 39/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5458     \n",
      "Epoch 40/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5531     \n",
      "Epoch 41/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5539     \n",
      "Epoch 42/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5428     \n",
      "Epoch 43/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5381     \n",
      "Epoch 44/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5408     \n",
      "Epoch 45/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5408     \n",
      "Epoch 46/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5382     \n",
      "Epoch 47/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5393     \n",
      "Epoch 48/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5321     \n",
      "Epoch 49/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5328     \n",
      "Epoch 50/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5323     \n",
      "Epoch 51/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5278     \n",
      "Epoch 52/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5271     \n",
      "Epoch 53/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5323     \n",
      "Epoch 54/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5244     \n",
      "Epoch 55/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5308     \n",
      "Epoch 56/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5218     \n",
      "Epoch 57/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5272     \n",
      "Epoch 58/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5230     \n",
      "Epoch 59/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5186     \n",
      "Epoch 60/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5238     \n",
      "Epoch 61/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5128     \n",
      "Epoch 62/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5229     \n",
      "Epoch 63/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5157     \n",
      "Epoch 64/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5172     \n",
      "Epoch 65/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5156     \n",
      "Epoch 66/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5182     \n",
      "Epoch 67/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5124     \n",
      "Epoch 68/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5162     \n",
      "Epoch 69/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5071     \n",
      "Epoch 70/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5100     \n",
      "Epoch 71/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5094     \n",
      "Epoch 72/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5083     \n",
      "Epoch 73/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5108     \n",
      "Epoch 74/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5061     \n",
      "Epoch 75/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5086     \n",
      "Epoch 76/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5055     \n",
      "Epoch 77/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5076     \n",
      "Epoch 78/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5070     \n",
      "Epoch 79/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5077     \n",
      "Epoch 80/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5017     \n",
      "Epoch 81/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5092     \n",
      "Epoch 82/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5023     \n",
      "Epoch 83/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5094     \n",
      "Epoch 84/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5042     \n",
      "Epoch 85/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5038     \n",
      "Epoch 86/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5032     \n",
      "Epoch 87/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5026     \n",
      "Epoch 88/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4994     \n",
      "Epoch 89/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5022     \n",
      "Epoch 90/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5004     \n",
      "Epoch 91/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4996     \n",
      "Epoch 92/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4986     \n",
      "Epoch 93/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4983     \n",
      "Epoch 94/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4981     \n",
      "Epoch 95/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4949     \n",
      "Epoch 96/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4924     \n",
      "Epoch 97/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.5016     \n",
      "Epoch 98/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4946     \n",
      "Epoch 99/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4954     \n",
      "Epoch 100/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4948     \n",
      "Epoch 101/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4931     \n",
      "Epoch 102/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4964     \n",
      "Epoch 103/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4973     \n",
      "Epoch 104/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4890     \n",
      "Epoch 105/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4910     \n",
      "Epoch 106/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4969     \n",
      "Epoch 107/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4901     \n",
      "Epoch 108/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4910     \n",
      "Epoch 109/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4908     \n",
      "Epoch 110/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4859     \n",
      "Epoch 111/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4884     \n",
      "Epoch 112/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4938     \n",
      "Epoch 113/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4838     \n",
      "Epoch 114/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4853     \n",
      "Epoch 115/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4924     \n",
      "Epoch 116/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4882     \n",
      "Epoch 117/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4840     \n",
      "Epoch 118/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4895     \n",
      "Epoch 119/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4871     \n",
      "Epoch 120/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4858     \n",
      "Epoch 121/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4825     \n",
      "Epoch 122/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4903     \n",
      "Epoch 123/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4854     \n",
      "Epoch 124/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4872     \n",
      "Epoch 125/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4826     \n",
      "Epoch 126/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4848     \n",
      "Epoch 127/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4839     \n",
      "Epoch 128/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4897     \n",
      "Epoch 129/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4804     \n",
      "Epoch 130/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4865     \n",
      "Epoch 131/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4850     \n",
      "Epoch 132/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4785     \n",
      "Epoch 133/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4907     \n",
      "Epoch 134/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 1s - loss: 0.4834     \n",
      "Epoch 135/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4801     \n",
      "Epoch 136/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4807     \n",
      "Epoch 137/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4816     \n",
      "Epoch 138/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4805     \n",
      "Epoch 139/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4819     \n",
      "Epoch 140/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4815     \n",
      "Epoch 141/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4847     \n",
      "Epoch 142/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4828     \n",
      "Epoch 143/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4819     \n",
      "Epoch 144/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4843     \n",
      "Epoch 145/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4806     \n",
      "Epoch 146/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4840     \n",
      "Epoch 147/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4811     \n",
      "Epoch 148/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4803     \n",
      "Epoch 149/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4766     \n",
      "Epoch 150/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4791     \n",
      "Epoch 151/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4740     \n",
      "Epoch 152/160\n",
      "8186/8186 [==============================] - ETA: 0s - loss: 0.470 - 1s - loss: 0.4738     \n",
      "Epoch 153/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4769     \n",
      "Epoch 154/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4775     \n",
      "Epoch 155/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4778     \n",
      "Epoch 156/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4771     \n",
      "Epoch 157/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4761     \n",
      "Epoch 158/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4760     \n",
      "Epoch 159/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4769     \n",
      "Epoch 160/160\n",
      "8186/8186 [==============================] - 1s - loss: 0.4746     \n",
      "128/910 [===>..........................] - ETA: 3s1.88265609898\n",
      "Squared loss 197719003.818\n",
      "RMSE 14061.2589699\n",
      "Average error 10.8013503603\n",
      "*****\n",
      "[ 2000.59936523] 1995\n",
      "[ 2000.59936523] 1997\n",
      "[ 2000.59936523] 1991\n",
      "[ 2000.59936523] 2008\n",
      "[ 2000.59936523] 2008\n",
      "[ 1992.61303711] 2008\n",
      "[ 2000.59936523] 2006\n",
      "[ 1970.82751465] 2006\n",
      "[ 1981.70861816] 2008\n",
      "[ 2000.27746582] 2008\n",
      "[ 2000.59936523] 1972\n",
      "[ 1995.7142334] 1972\n",
      "[ 2000.59936523] 1993\n",
      "[ 2000.59936523] 1998\n",
      "[ 2000.59936523] 1993\n",
      "************* 256 *************\n",
      "[<keras.layers.core.Dense object at 0x12b99de90>, <keras.layers.core.Activation object at 0x12b8517d0>, <keras.layers.core.Dense object at 0x12b851f10>]\n",
      "Epoch 1/160\n",
      "8186/8186 [==============================] - 3s - loss: 1760.4162     \n",
      "Epoch 2/160\n",
      "8186/8186 [==============================] - 2s - loss: 19.2525     \n",
      "Epoch 3/160\n",
      "8186/8186 [==============================] - 2s - loss: 6.0488     \n",
      "Epoch 4/160\n",
      "8186/8186 [==============================] - 2s - loss: 2.8520     \n",
      "Epoch 5/160\n",
      "8186/8186 [==============================] - 2s - loss: 1.0171     \n",
      "Epoch 6/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.9882     \n",
      "Epoch 7/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.9253     \n",
      "Epoch 8/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.8790     \n",
      "Epoch 9/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.8326     \n",
      "Epoch 10/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.7953     \n",
      "Epoch 11/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.7816     \n",
      "Epoch 12/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.7500     \n",
      "Epoch 13/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.7214     \n",
      "Epoch 14/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.6980     \n",
      "Epoch 15/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.6793     \n",
      "Epoch 16/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.6513     \n",
      "Epoch 17/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.6369     \n",
      "Epoch 18/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.6156     \n",
      "Epoch 19/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5991     \n",
      "Epoch 20/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5947     \n",
      "Epoch 21/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.6020     \n",
      "Epoch 22/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5742     \n",
      "Epoch 23/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5578     \n",
      "Epoch 24/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5487     \n",
      "Epoch 25/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5435     \n",
      "Epoch 26/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5315     \n",
      "Epoch 27/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5369     \n",
      "Epoch 28/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5200     \n",
      "Epoch 29/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5049     \n",
      "Epoch 30/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.5076     \n",
      "Epoch 31/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4974     \n",
      "Epoch 32/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4867     \n",
      "Epoch 33/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4842     \n",
      "Epoch 34/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4821     \n",
      "Epoch 35/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4884     \n",
      "Epoch 36/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4745     \n",
      "Epoch 37/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4664     \n",
      "Epoch 38/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4634     \n",
      "Epoch 39/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4641     \n",
      "Epoch 40/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4603     \n",
      "Epoch 41/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4583     \n",
      "Epoch 42/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4503     \n",
      "Epoch 43/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4504     \n",
      "Epoch 44/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4519     \n",
      "Epoch 45/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4428     \n",
      "Epoch 46/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4493     \n",
      "Epoch 47/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4311     \n",
      "Epoch 48/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4350     \n",
      "Epoch 49/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4332     \n",
      "Epoch 50/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4329     \n",
      "Epoch 51/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4284     \n",
      "Epoch 52/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4294     \n",
      "Epoch 53/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4192     \n",
      "Epoch 54/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4201     \n",
      "Epoch 55/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4234     \n",
      "Epoch 56/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4209     \n",
      "Epoch 57/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4109     \n",
      "Epoch 58/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4116     \n",
      "Epoch 59/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4037     \n",
      "Epoch 60/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4025     \n",
      "Epoch 61/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4065     \n",
      "Epoch 62/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4024     \n",
      "Epoch 63/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.4027     \n",
      "Epoch 64/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3953     \n",
      "Epoch 65/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3968     \n",
      "Epoch 66/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 2s - loss: 0.3932     \n",
      "Epoch 67/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3926     \n",
      "Epoch 68/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3978     \n",
      "Epoch 69/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3940     \n",
      "Epoch 70/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3943     \n",
      "Epoch 71/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3884     \n",
      "Epoch 72/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3876     \n",
      "Epoch 73/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3864     \n",
      "Epoch 74/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3857     \n",
      "Epoch 75/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3831     \n",
      "Epoch 76/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3867     \n",
      "Epoch 77/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3849     \n",
      "Epoch 78/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3769     \n",
      "Epoch 79/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3786     \n",
      "Epoch 80/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3727     \n",
      "Epoch 81/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3784     \n",
      "Epoch 82/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3787     \n",
      "Epoch 83/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3739     \n",
      "Epoch 84/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3749     \n",
      "Epoch 85/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3727     \n",
      "Epoch 86/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3759     \n",
      "Epoch 87/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3733     \n",
      "Epoch 88/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3692     \n",
      "Epoch 89/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3717     \n",
      "Epoch 90/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3710     \n",
      "Epoch 91/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3687     \n",
      "Epoch 92/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3663     \n",
      "Epoch 93/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3652     \n",
      "Epoch 94/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3701     \n",
      "Epoch 95/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3665     \n",
      "Epoch 96/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3677     \n",
      "Epoch 97/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3690     \n",
      "Epoch 98/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3655     \n",
      "Epoch 99/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3608     \n",
      "Epoch 100/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3618     \n",
      "Epoch 101/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3673     \n",
      "Epoch 102/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3678     \n",
      "Epoch 103/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3616     \n",
      "Epoch 104/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3600     \n",
      "Epoch 105/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3615     \n",
      "Epoch 106/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3606     \n",
      "Epoch 107/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3596     \n",
      "Epoch 108/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3546     \n",
      "Epoch 109/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3704     \n",
      "Epoch 110/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3617     \n",
      "Epoch 111/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3540     \n",
      "Epoch 112/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3534     \n",
      "Epoch 113/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3583     \n",
      "Epoch 114/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3564     \n",
      "Epoch 115/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3545     \n",
      "Epoch 116/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3564     \n",
      "Epoch 117/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3534     \n",
      "Epoch 118/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3546     \n",
      "Epoch 119/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3581     \n",
      "Epoch 120/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3514     \n",
      "Epoch 121/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3522     \n",
      "Epoch 122/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3568     \n",
      "Epoch 123/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3500     \n",
      "Epoch 124/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3523     \n",
      "Epoch 125/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3507     \n",
      "Epoch 126/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3459     \n",
      "Epoch 127/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3503     \n",
      "Epoch 128/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3463     \n",
      "Epoch 129/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3514     \n",
      "Epoch 130/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3477     \n",
      "Epoch 131/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3480     \n",
      "Epoch 132/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3483     \n",
      "Epoch 133/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3469     \n",
      "Epoch 134/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3483     \n",
      "Epoch 135/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3501     \n",
      "Epoch 136/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3426     \n",
      "Epoch 137/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3488     \n",
      "Epoch 138/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3517     \n",
      "Epoch 139/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3447     \n",
      "Epoch 140/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3449     \n",
      "Epoch 141/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3474     \n",
      "Epoch 142/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3409     \n",
      "Epoch 143/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3416     \n",
      "Epoch 144/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3422     \n",
      "Epoch 145/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3512     \n",
      "Epoch 146/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3456     \n",
      "Epoch 147/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3422     \n",
      "Epoch 148/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3376     \n",
      "Epoch 149/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3509     \n",
      "Epoch 150/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3422     \n",
      "Epoch 151/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3359     \n",
      "Epoch 152/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3441     \n",
      "Epoch 153/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3396     \n",
      "Epoch 154/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3387     \n",
      "Epoch 155/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3342     \n",
      "Epoch 156/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3370     \n",
      "Epoch 157/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3416     \n",
      "Epoch 158/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3439     \n",
      "Epoch 159/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3423     \n",
      "Epoch 160/160\n",
      "8186/8186 [==============================] - 2s - loss: 0.3392     \n",
      "128/910 [===>..........................] - ETA: 3s1.99111330064\n",
      "Squared loss 213304854.3\n",
      "RMSE 14604.9599212\n",
      "Average error 11.9698543638\n",
      "*****\n",
      "[ 2003.97888184] 1995\n",
      "[ 1997.78125] 1997\n",
      "[ 1997.71594238] 1991\n",
      "[ 1994.4942627] 2008\n",
      "[ 1992.70898438] 2008\n",
      "[ 1992.70898438] 2008\n",
      "[ 2009.53210449] 2006\n",
      "[ 1983.52844238] 2006\n",
      "[ 1962.86035156] 2008\n",
      "[ 1986.26464844] 2008\n",
      "[ 1998.04760742] 1972\n",
      "[ 1983.19140625] 1972\n",
      "[ 1994.99768066] 1993\n",
      "[ 1992.70898438] 1998\n",
      "[ 2002.55773926] 1993\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "print flat_X_train.shape[1]\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "y_sd = np.std(y_train)\n",
    "\n",
    "y_train_norm = (y_train - y_mean) / y_sd\n",
    "y_test_norm = (y_test - y_mean) / y_sd\n",
    "\n",
    "for N_UNITS in [8,16,32,64,128,256]:\n",
    "    print '*************', N_UNITS, '*************'\n",
    "    model = None\n",
    "    model = Sequential()\n",
    "    model.add(Dense(N_UNITS, input_dim=2400))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    print model.layers\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='mse')\n",
    "    model.fit(flat_X_train, y_train_norm, epochs=160, batch_size=32, verbose=1)\n",
    "    \n",
    "    score = model.evaluate(flat_X_test, y_test_norm, batch_size=128)\n",
    "    print score\n",
    "    \n",
    "    t_norm = model.predict(flat_X_test)\n",
    "    t = t_norm * y_sd + y_mean\n",
    "    sl = np.sum((t - y_test)**2)\n",
    "    \n",
    "    print 'Squared loss',sl\n",
    "    print 'RMSE', np.sqrt(sl)\n",
    "    print 'Average error', np.mean(np.abs(t - y_test))\n",
    "    print('*****')\n",
    "    for i in range(15):\n",
    "        print t[i], y_train[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 3: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "************* 8 *************\n",
      "[<keras.layers.recurrent.LSTM object at 0x12b14e950>, <keras.layers.core.Dense object at 0x12b14ec50>]\n",
      "Epoch 1/50\n",
      "8186/8186 [==============================] - 144s - loss: 1.0760   \n",
      "Epoch 2/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9955   \n",
      "Epoch 3/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9699   \n",
      "Epoch 4/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9544   \n",
      "Epoch 5/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9451   \n",
      "Epoch 6/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9395   \n",
      "Epoch 7/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9371   \n",
      "Epoch 8/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9321   \n",
      "Epoch 9/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9271   \n",
      "Epoch 10/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9248   \n",
      "Epoch 11/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9246   \n",
      "Epoch 12/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9215   \n",
      "Epoch 13/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9201   \n",
      "Epoch 14/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9162   \n",
      "Epoch 15/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9171   \n",
      "Epoch 16/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9129   \n",
      "Epoch 17/50\n",
      "8186/8186 [==============================] - 143s - loss: 0.9141   \n",
      "Epoch 18/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9112   \n",
      "Epoch 19/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.9085   \n",
      "Epoch 20/50\n",
      "8186/8186 [==============================] - 147s - loss: 0.9083   \n",
      "Epoch 21/50\n",
      "8186/8186 [==============================] - 148s - loss: 0.9065   \n",
      "Epoch 22/50\n",
      "8186/8186 [==============================] - 148s - loss: 0.9054   \n",
      "Epoch 23/50\n",
      "8186/8186 [==============================] - 146s - loss: 0.9042   \n",
      "Epoch 24/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.9014   \n",
      "Epoch 25/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8987   \n",
      "Epoch 26/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8973   \n",
      "Epoch 27/50\n",
      "8186/8186 [==============================] - 141s - loss: 0.8959   \n",
      "Epoch 28/50\n",
      "8186/8186 [==============================] - 141s - loss: 0.8942   \n",
      "Epoch 29/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8930   \n",
      "Epoch 30/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8934   \n",
      "Epoch 31/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8925   \n",
      "Epoch 32/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8910   \n",
      "Epoch 33/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8891   \n",
      "Epoch 34/50\n",
      "8186/8186 [==============================] - 143s - loss: 0.8881   \n",
      "Epoch 35/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8864   \n",
      "Epoch 36/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8855   \n",
      "Epoch 37/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8834   \n",
      "Epoch 38/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8830   \n",
      "Epoch 39/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8832   \n",
      "Epoch 40/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8816   \n",
      "Epoch 41/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8814   \n",
      "Epoch 42/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8816   \n",
      "Epoch 43/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8770   \n",
      "Epoch 44/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8782   \n",
      "Epoch 45/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8790   \n",
      "Epoch 46/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8762   \n",
      "Epoch 47/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8785   \n",
      "Epoch 48/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8767   \n",
      "Epoch 49/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8736   \n",
      "Epoch 50/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8735   \n",
      "910/910 [==============================] - 1s     \n",
      "1.09108163839\n",
      "Squared loss 126592549.405\n",
      "RMSE 11251.3354499\n",
      "Average error 9.58664036792\n",
      "*****\n",
      "[ 1995.36694336] 1995\n",
      "[ 1999.76428223] 1997\n",
      "[ 2000.54064941] 1991\n",
      "[ 1999.14404297] 2008\n",
      "[ 1998.59545898] 2008\n",
      "[ 1994.31506348] 2008\n",
      "[ 1999.88049316] 2006\n",
      "[ 1999.87939453] 2006\n",
      "[ 1999.14770508] 2008\n",
      "[ 1990.94702148] 2008\n",
      "[ 1992.57531738] 1972\n",
      "[ 1994.88720703] 1972\n",
      "[ 1996.68212891] 1993\n",
      "[ 1993.37756348] 1998\n",
      "[ 2003.7734375] 1993\n",
      "************* 16 *************\n",
      "[<keras.layers.recurrent.LSTM object at 0x12f9f5ed0>, <keras.layers.core.Dense object at 0x12bfc9510>]\n",
      "Epoch 1/50\n",
      "8186/8186 [==============================] - 146s - loss: 1.2506   \n",
      "Epoch 2/50\n",
      "8186/8186 [==============================] - 144s - loss: 1.0296   \n",
      "Epoch 3/50\n",
      "8186/8186 [==============================] - 140s - loss: 0.9650   \n",
      "Epoch 4/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.9350   \n",
      "Epoch 5/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.9149   \n",
      "Epoch 6/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.9009   \n",
      "Epoch 7/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.8920   \n",
      "Epoch 8/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8864   \n",
      "Epoch 9/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8803   \n",
      "Epoch 10/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8743   \n",
      "Epoch 11/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.8695   \n",
      "Epoch 12/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8685   \n",
      "Epoch 13/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8607   \n",
      "Epoch 14/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8550   \n",
      "Epoch 15/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8472   \n",
      "Epoch 16/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8449   \n",
      "Epoch 17/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8416   \n",
      "Epoch 18/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.8375   \n",
      "Epoch 19/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.8354   \n",
      "Epoch 20/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8317   \n",
      "Epoch 21/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8313   \n",
      "Epoch 22/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8278   \n",
      "Epoch 23/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8254   \n",
      "Epoch 24/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8239   \n",
      "Epoch 25/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8193   \n",
      "Epoch 26/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8188   \n",
      "Epoch 27/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8163   \n",
      "Epoch 28/50\n",
      "8186/8186 [==============================] - 142s - loss: 0.8147   \n",
      "Epoch 29/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8119   \n",
      "Epoch 30/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8119   \n",
      "Epoch 31/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.8109   \n",
      "Epoch 32/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8084   \n",
      "Epoch 33/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.8105   \n",
      "Epoch 34/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.8086   \n",
      "Epoch 35/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.8084   \n",
      "Epoch 36/50\n",
      "8186/8186 [==============================] - 145s - loss: 0.8073   \n",
      "Epoch 37/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8036   \n",
      "Epoch 38/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.7993   \n",
      "Epoch 39/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.8020   \n",
      "Epoch 40/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.7994   \n",
      "Epoch 41/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.7978   \n",
      "Epoch 42/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.7935   \n",
      "Epoch 43/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.7964   \n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 144s - loss: 0.7990   \n",
      "Epoch 45/50\n",
      "8186/8186 [==============================] - 143s - loss: 0.7948   \n",
      "Epoch 46/50\n",
      "8186/8186 [==============================] - 143s - loss: 0.7918   \n",
      "Epoch 47/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.7956   \n",
      "Epoch 48/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.7930   \n",
      "Epoch 49/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.7900   \n",
      "Epoch 50/50\n",
      "8186/8186 [==============================] - 144s - loss: 0.7903   \n",
      "910/910 [==============================] - 1s     \n",
      "1.11248284775\n",
      "Squared loss 137268923.36\n",
      "RMSE 11716.1821153\n",
      "Average error 9.94535016004\n",
      "*****\n",
      "[ 1997.76928711] 1995\n",
      "[ 2000.78137207] 1997\n",
      "[ 2000.98095703] 1991\n",
      "[ 1994.3458252] 2008\n",
      "[ 1995.7442627] 2008\n",
      "[ 1999.4498291] 2008\n",
      "[ 2002.15356445] 2006\n",
      "[ 1997.6854248] 2006\n",
      "[ 1994.23388672] 2008\n",
      "[ 1991.93444824] 2008\n",
      "[ 1985.6940918] 1972\n",
      "[ 1998.30627441] 1972\n",
      "[ 1991.91748047] 1993\n",
      "[ 1991.87097168] 1998\n",
      "[ 2004.77087402] 1993\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "print flat_X_train.shape[1]\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "y_sd = np.std(y_train)\n",
    "\n",
    "y_train_norm = (y_train - y_mean) / y_sd\n",
    "y_test_norm = (y_test - y_mean) / y_sd\n",
    "\n",
    "for N_UNITS in [8, 16]:\n",
    "    print '*************', N_UNITS, '*************'\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(N_UNITS, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    print model.layers\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='mse')\n",
    "    model.fit(X_train, y_train_norm, epochs=50, batch_size=32, verbose=1)\n",
    "    \n",
    "    score = model.evaluate(X_test, y_test_norm, batch_size=128)\n",
    "    print score\n",
    "    \n",
    "    t_norm = model.predict(X_test)\n",
    "    t = t_norm * y_sd + y_mean\n",
    "    sl = np.sum((t - y_test)**2)\n",
    "    \n",
    "    print 'Squared loss',sl\n",
    "    print 'RMSE', np.sqrt(sl)\n",
    "    print 'Average error', np.mean(np.abs(t - y_test))\n",
    "    print('*****')\n",
    "    for i in range(15):\n",
    "        print t[i], y_train[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 4: Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "************* 8 *************\n",
      "(None, 197, 8)\n",
      "(None, 8)\n",
      "(None, 1)\n",
      "Epoch 1/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.4732     \n",
      "Epoch 2/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.3566     \n",
      "Epoch 3/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.2692     \n",
      "Epoch 4/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.2673     \n",
      "Epoch 5/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.2304     \n",
      "Epoch 6/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.1870     \n",
      "Epoch 7/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.1820     \n",
      "Epoch 8/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.1639     \n",
      "Epoch 9/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.1438     \n",
      "Epoch 10/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0869     \n",
      "Epoch 11/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.1151     \n",
      "Epoch 12/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0791     \n",
      "Epoch 13/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0780     \n",
      "Epoch 14/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0637     \n",
      "Epoch 15/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0420     \n",
      "Epoch 16/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0236     \n",
      "Epoch 17/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0213     \n",
      "Epoch 18/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0038     \n",
      "Epoch 19/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9914     \n",
      "Epoch 20/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9850     \n",
      "Epoch 21/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9824     \n",
      "Epoch 22/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9720     \n",
      "Epoch 23/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9632     \n",
      "Epoch 24/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9608     \n",
      "Epoch 25/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9612     \n",
      "Epoch 26/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9536     \n",
      "Epoch 27/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9482     \n",
      "Epoch 28/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9431     \n",
      "Epoch 29/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9389     \n",
      "Epoch 30/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9385     \n",
      "Epoch 31/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9353     \n",
      "Epoch 32/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9317     \n",
      "Epoch 33/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9293     \n",
      "Epoch 34/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9238     \n",
      "Epoch 35/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9223     \n",
      "Epoch 36/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9197     \n",
      "Epoch 37/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9178     \n",
      "Epoch 38/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9121     \n",
      "Epoch 39/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9107     \n",
      "Epoch 40/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9113     \n",
      "Epoch 41/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9084     \n",
      "Epoch 42/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9031     \n",
      "Epoch 43/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9007     \n",
      "Epoch 44/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9022     \n",
      "Epoch 45/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9004     \n",
      "Epoch 46/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8974     \n",
      "Epoch 47/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8985     \n",
      "Epoch 48/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8990     \n",
      "Epoch 49/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8957     \n",
      "Epoch 50/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8894     \n",
      "128/910 [===>..........................] - ETA: 2s0.989629048306\n",
      "Squared loss 122861234.979\n",
      "RMSE 11084.2787307\n",
      "Average error 9.51684703217\n",
      "*****\n",
      "[ 1998.7545166] 1995\n",
      "[ 1998.30102539] 1997\n",
      "[ 1998.24731445] 1991\n",
      "[ 1995.50793457] 2008\n",
      "[ 1999.1340332] 2008\n",
      "[ 1996.07568359] 2008\n",
      "[ 1998.49682617] 2006\n",
      "[ 1998.58227539] 2006\n",
      "[ 1998.05163574] 2008\n",
      "[ 1997.29394531] 2008\n",
      "[ 1994.86804199] 1972\n",
      "[ 1996.18457031] 1972\n",
      "[ 1996.99121094] 1993\n",
      "[ 1997.86682129] 1998\n",
      "[ 1999.78515625] 1993\n",
      "************* 16 *************\n",
      "(None, 197, 16)\n",
      "(None, 16)\n",
      "(None, 1)\n",
      "Epoch 1/50\n",
      "8186/8186 [==============================] - 1s - loss: 2.3732     \n",
      "Epoch 2/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.6006     \n",
      "Epoch 3/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.5129     \n",
      "Epoch 4/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.3926     \n",
      "Epoch 5/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.3021     \n",
      "Epoch 6/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.2667     \n",
      "Epoch 7/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.2052     \n",
      "Epoch 8/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.1697     \n",
      "Epoch 9/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.1333     \n",
      "Epoch 10/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.1144     \n",
      "Epoch 11/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0897     \n",
      "Epoch 12/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0613     \n",
      "Epoch 13/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0450     \n",
      "Epoch 14/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0263     \n",
      "Epoch 15/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0047     \n",
      "Epoch 16/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9885     \n",
      "Epoch 17/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9715     \n",
      "Epoch 18/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9600     \n",
      "Epoch 19/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9596     \n",
      "Epoch 20/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9420     \n",
      "Epoch 21/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9407     \n",
      "Epoch 22/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9320     \n",
      "Epoch 23/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9264     \n",
      "Epoch 24/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9272     \n",
      "Epoch 25/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9196     \n",
      "Epoch 26/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9183     \n",
      "Epoch 27/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9151     \n",
      "Epoch 28/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9114     \n",
      "Epoch 29/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9142     \n",
      "Epoch 30/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9116     \n",
      "Epoch 31/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9106     \n",
      "Epoch 32/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9091     \n",
      "Epoch 33/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9082     \n",
      "Epoch 34/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9083     \n",
      "Epoch 35/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9075     \n",
      "Epoch 36/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9076     \n",
      "Epoch 37/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9055     \n",
      "Epoch 38/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9045     \n",
      "Epoch 39/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9020     \n",
      "Epoch 40/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8979     \n",
      "Epoch 41/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9001     \n",
      "Epoch 42/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9053     \n",
      "Epoch 43/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9023     \n",
      "Epoch 44/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9043     \n",
      "Epoch 45/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8992     \n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 1s - loss: 0.8959     \n",
      "Epoch 47/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8960     \n",
      "Epoch 48/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8956     \n",
      "Epoch 49/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8946     \n",
      "Epoch 50/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8930     \n",
      "128/910 [===>..........................] - ETA: 2s0.984922089682\n",
      "Squared loss 122192867.8\n",
      "RMSE 11054.0882844\n",
      "Average error 9.36271194419\n",
      "*****\n",
      "[ 1998.77246094] 1995\n",
      "[ 1997.7623291] 1997\n",
      "[ 1999.03869629] 1991\n",
      "[ 1995.37780762] 2008\n",
      "[ 1998.95629883] 2008\n",
      "[ 1998.02587891] 2008\n",
      "[ 1998.58178711] 2006\n",
      "[ 1998.50061035] 2006\n",
      "[ 1998.17797852] 2008\n",
      "[ 1997.37280273] 2008\n",
      "[ 1994.73168945] 1972\n",
      "[ 1996.11877441] 1972\n",
      "[ 1997.62072754] 1993\n",
      "[ 1998.71582031] 1998\n",
      "[ 1997.84692383] 1993\n",
      "************* 32 *************\n",
      "(None, 197, 32)\n",
      "(None, 32)\n",
      "(None, 1)\n",
      "Epoch 1/50\n",
      "8186/8186 [==============================] - 2s - loss: 3.4126     \n",
      "Epoch 2/50\n",
      "8186/8186 [==============================] - 2s - loss: 2.5842     \n",
      "Epoch 3/50\n",
      "8186/8186 [==============================] - 1s - loss: 2.1658     \n",
      "Epoch 4/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.9228     \n",
      "Epoch 5/50\n",
      "8186/8186 [==============================] - 2s - loss: 1.7071     \n",
      "Epoch 6/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.4946     \n",
      "Epoch 7/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.4051     \n",
      "Epoch 8/50\n",
      "8186/8186 [==============================] - 2s - loss: 1.3065     \n",
      "Epoch 9/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.2336     \n",
      "Epoch 10/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.1634     \n",
      "Epoch 11/50\n",
      "8186/8186 [==============================] - 2s - loss: 1.0930     \n",
      "Epoch 12/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0601     \n",
      "Epoch 13/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0307     \n",
      "Epoch 14/50\n",
      "8186/8186 [==============================] - 1s - loss: 1.0092     \n",
      "Epoch 15/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9914     \n",
      "Epoch 16/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9684     \n",
      "Epoch 17/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9618     \n",
      "Epoch 18/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9490     \n",
      "Epoch 19/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9383     \n",
      "Epoch 20/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9310     \n",
      "Epoch 21/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9271     \n",
      "Epoch 22/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9264     \n",
      "Epoch 23/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9238     \n",
      "Epoch 24/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9213     \n",
      "Epoch 25/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9206     \n",
      "Epoch 26/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9173     \n",
      "Epoch 27/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9176     \n",
      "Epoch 28/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9175     \n",
      "Epoch 29/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9157     \n",
      "Epoch 30/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9160     \n",
      "Epoch 31/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9128     \n",
      "Epoch 32/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9129     \n",
      "Epoch 33/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9107     \n",
      "Epoch 34/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9104     \n",
      "Epoch 35/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9142     \n",
      "Epoch 36/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9117     \n",
      "Epoch 37/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9054     \n",
      "Epoch 38/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9049     \n",
      "Epoch 39/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9079     \n",
      "Epoch 40/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9034     \n",
      "Epoch 41/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9051     \n",
      "Epoch 42/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9033     \n",
      "Epoch 43/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9004     \n",
      "Epoch 44/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8987     \n",
      "Epoch 45/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.9001     \n",
      "Epoch 46/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8981     \n",
      "Epoch 47/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8972     \n",
      "Epoch 48/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8974     \n",
      "Epoch 49/50\n",
      "8186/8186 [==============================] - 1s - loss: 0.8937     \n",
      "Epoch 50/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.8985     \n",
      "896/910 [============================>.] - ETA: 0s0.992094521732\n",
      "Squared loss 122863591.668\n",
      "RMSE 11084.3850379\n",
      "Average error 9.24811769996\n",
      "*****\n",
      "[ 2000.88671875] 1995\n",
      "[ 2000.10314941] 1997\n",
      "[ 1999.88842773] 1991\n",
      "[ 1995.5501709] 2008\n",
      "[ 1998.46447754] 2008\n",
      "[ 1996.98461914] 2008\n",
      "[ 1998.78051758] 2006\n",
      "[ 1998.35473633] 2006\n",
      "[ 1998.0390625] 2008\n",
      "[ 1999.74133301] 2008\n",
      "[ 1996.28320312] 1972\n",
      "[ 1998.14880371] 1972\n",
      "[ 1997.52600098] 1993\n",
      "[ 1999.62695312] 1998\n",
      "[ 1998.71289062] 1993\n",
      "************* 64 *************\n",
      "(None, 197, 64)\n",
      "(None, 64)\n",
      "(None, 1)\n",
      "Epoch 1/50\n",
      "8186/8186 [==============================] - 2s - loss: 7.1601     \n",
      "Epoch 2/50\n",
      "8186/8186 [==============================] - 2s - loss: 4.7746     \n",
      "Epoch 3/50\n",
      "8186/8186 [==============================] - 2s - loss: 3.4135     \n",
      "Epoch 4/50\n",
      "8186/8186 [==============================] - 2s - loss: 2.3598     \n",
      "Epoch 5/50\n",
      "8186/8186 [==============================] - 2s - loss: 1.7600     \n",
      "Epoch 6/50\n",
      "8186/8186 [==============================] - 2s - loss: 1.4501     \n",
      "Epoch 7/50\n",
      "8186/8186 [==============================] - 2s - loss: 1.2237     \n",
      "Epoch 8/50\n",
      "8186/8186 [==============================] - 2s - loss: 1.0984     \n",
      "Epoch 9/50\n",
      "8186/8186 [==============================] - 2s - loss: 1.0252     \n",
      "Epoch 10/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9797     \n",
      "Epoch 11/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9642     \n",
      "Epoch 12/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9503     \n",
      "Epoch 13/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9443     \n",
      "Epoch 14/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9427     \n",
      "Epoch 15/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9421     \n",
      "Epoch 16/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9344     \n",
      "Epoch 17/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9374     \n",
      "Epoch 18/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9357     \n",
      "Epoch 19/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9345     \n",
      "Epoch 20/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9301     \n",
      "Epoch 21/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9304     \n",
      "Epoch 22/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9320     \n",
      "Epoch 23/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9280     \n",
      "Epoch 24/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9294     \n",
      "Epoch 25/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9361     \n",
      "Epoch 26/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9286     \n",
      "Epoch 27/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9267     \n",
      "Epoch 28/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9298     \n",
      "Epoch 29/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9280     \n",
      "Epoch 30/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9327     \n",
      "Epoch 31/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9261     \n",
      "Epoch 32/50\n",
      "8186/8186 [==============================] - ETA: 0s - loss: 0.922 - 2s - loss: 0.9281     \n",
      "Epoch 33/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9245     \n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8186/8186 [==============================] - 2s - loss: 0.9161     \n",
      "Epoch 35/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9133     \n",
      "Epoch 36/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9114     \n",
      "Epoch 37/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9098     \n",
      "Epoch 38/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9053     \n",
      "Epoch 39/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9061     \n",
      "Epoch 40/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9059     \n",
      "Epoch 41/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9073     \n",
      "Epoch 42/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9039     \n",
      "Epoch 43/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9048     \n",
      "Epoch 44/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9051     \n",
      "Epoch 45/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9016     \n",
      "Epoch 46/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9033     \n",
      "Epoch 47/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9005     \n",
      "Epoch 48/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.9016     \n",
      "Epoch 49/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.8992     \n",
      "Epoch 50/50\n",
      "8186/8186 [==============================] - 2s - loss: 0.8997     \n",
      "640/910 [====================>.........] - ETA: 0s0.997115978304\n",
      "Squared loss 121411754.767\n",
      "RMSE 11018.7002304\n",
      "Average error 9.37915932309\n",
      "*****\n",
      "[ 1997.32543945] 1995\n",
      "[ 1996.87670898] 1997\n",
      "[ 1998.4230957] 1991\n",
      "[ 1995.59411621] 2008\n",
      "[ 1999.171875] 2008\n",
      "[ 1997.78771973] 2008\n",
      "[ 1998.90283203] 2006\n",
      "[ 1998.515625] 2006\n",
      "[ 1998.2097168] 2008\n",
      "[ 1997.62512207] 2008\n",
      "[ 1996.06958008] 1972\n",
      "[ 1996.71899414] 1972\n",
      "[ 1997.67150879] 1993\n",
      "[ 1998.78393555] 1998\n",
      "[ 1997.94726562] 1993\n",
      "************* 128 *************\n",
      "(None, 197, 128)\n",
      "(None, 128)\n",
      "(None, 1)\n",
      "Epoch 1/50\n",
      "8186/8186 [==============================] - 4s - loss: 8.3392     \n",
      "Epoch 2/50\n",
      "8186/8186 [==============================] - 4s - loss: 3.0119     \n",
      "Epoch 3/50\n",
      "8186/8186 [==============================] - 4s - loss: 1.5954     \n",
      "Epoch 4/50\n",
      "8186/8186 [==============================] - 4s - loss: 1.1927     \n",
      "Epoch 5/50\n",
      "8186/8186 [==============================] - 4s - loss: 1.0401     \n",
      "Epoch 6/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9826     \n",
      "Epoch 7/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9610     \n",
      "Epoch 8/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9495     \n",
      "Epoch 9/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9468     \n",
      "Epoch 10/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9441     \n",
      "Epoch 11/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9436     \n",
      "Epoch 12/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9423     \n",
      "Epoch 13/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9369     \n",
      "Epoch 14/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9377     \n",
      "Epoch 15/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9291     \n",
      "Epoch 16/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9377     \n",
      "Epoch 17/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9336     \n",
      "Epoch 18/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9333     \n",
      "Epoch 19/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9284     \n",
      "Epoch 20/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9339     \n",
      "Epoch 21/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9331     \n",
      "Epoch 22/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9290     \n",
      "Epoch 23/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9315     \n",
      "Epoch 24/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9273     \n",
      "Epoch 25/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9256     \n",
      "Epoch 26/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9245     \n",
      "Epoch 27/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9243     \n",
      "Epoch 28/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9268     \n",
      "Epoch 29/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9266     \n",
      "Epoch 30/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9204     \n",
      "Epoch 31/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9239     \n",
      "Epoch 32/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9208     \n",
      "Epoch 33/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9156     \n",
      "Epoch 34/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9148     \n",
      "Epoch 35/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9202     \n",
      "Epoch 36/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9153     \n",
      "Epoch 37/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9139     \n",
      "Epoch 38/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9131     \n",
      "Epoch 39/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9130     \n",
      "Epoch 40/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9133     \n",
      "Epoch 41/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9126     \n",
      "Epoch 42/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9085     \n",
      "Epoch 43/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9213     \n",
      "Epoch 44/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9153     \n",
      "Epoch 45/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9069     \n",
      "Epoch 46/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9189     \n",
      "Epoch 47/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9106     \n",
      "Epoch 48/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9112     \n",
      "Epoch 49/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9119     \n",
      "Epoch 50/50\n",
      "8186/8186 [==============================] - 4s - loss: 0.9148     \n",
      "896/910 [============================>.] - ETA: 0s1.06750932133\n",
      "Squared loss 121977954.525\n",
      "RMSE 11044.3630203\n",
      "Average error 9.31026423557\n",
      "*****\n",
      "[ 1998.1036377] 1995\n",
      "[ 1998.6171875] 1997\n",
      "[ 1998.50195312] 1991\n",
      "[ 1994.99902344] 2008\n",
      "[ 1997.12841797] 2008\n",
      "[ 1996.88696289] 2008\n",
      "[ 1997.90563965] 2006\n",
      "[ 1997.6829834] 2006\n",
      "[ 1998.18432617] 2008\n",
      "[ 2001.76489258] 2008\n",
      "[ 1999.07263184] 1972\n",
      "[ 2001.35131836] 1972\n",
      "[ 1996.23095703] 1993\n",
      "[ 1999.08740234] 1998\n",
      "[ 1994.79956055] 1993\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, GlobalAveragePooling1D\n",
    "print flat_X_train.shape[1]\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "y_sd = np.std(y_train)\n",
    "\n",
    "y_train_norm = (y_train - y_mean) / y_sd\n",
    "y_test_norm = (y_test - y_mean) / y_sd\n",
    "\n",
    "for N_UNITS in [8, 16, 32, 64, 128]:\n",
    "    print '*************', N_UNITS, '*************'\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(N_UNITS, 4, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        print layer.output_shape\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='mse')\n",
    "    model.fit(X_train, y_train_norm, epochs=50, batch_size=32, verbose=1)\n",
    "    \n",
    "    score = model.evaluate(X_test, y_test_norm, batch_size=128)\n",
    "    print score\n",
    "    \n",
    "    t_norm = model.predict(X_test)\n",
    "    t = t_norm * y_sd + y_mean\n",
    "    sl = np.sum((t - y_test)**2)\n",
    "    \n",
    "    print 'Squared loss',sl\n",
    "    print 'RMSE', np.sqrt(sl)\n",
    "    print 'Average error', np.mean(np.abs(t - y_test))\n",
    "    print('*****')\n",
    "    for i in range(15):\n",
    "        print t[i], y_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
